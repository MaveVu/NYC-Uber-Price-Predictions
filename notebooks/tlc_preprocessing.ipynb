{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import unix_timestamp, hour, to_date, dayofweek, when\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/08/21 14:38:40 WARN Utils: Your hostname, MinhVu resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/08/21 14:38:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/21 14:38:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/21 14:38:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"tlc_preprocessing\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using lower case convention for the columns' names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename(year, month):\n",
    "    INPUT_PATH = '../data/landing/'\n",
    "    month = str(month).zfill(2)\n",
    "    sdf = spark.read.parquet(f'{INPUT_PATH}{year}-{month}.parquet')\n",
    "    lower_cols = [c.lower() for c in sdf.columns]\n",
    "    new_cols = dict(zip(sdf.columns, lower_cols))\n",
    "    for key, value in new_cols.items():\n",
    "        sdf = sdf.withColumnRenamed(key, value)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin rename 2023-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End rename 2023-12\n",
      "Begin rename 2024-01\n",
      "End rename 2024-01\n",
      "Begin rename 2024-02\n",
      "End rename 2024-02\n",
      "Begin rename 2024-03\n",
      "End rename 2024-03\n",
      "Begin rename 2024-04\n",
      "End rename 2024-04\n",
      "Begin rename 2024-05\n",
      "End rename 2024-05\n"
     ]
    }
   ],
   "source": [
    "data = {2023: [12], 2024: [1,2,3,4,5]}\n",
    "sdfs = {}\n",
    "\n",
    "for year in data.keys():\n",
    "    for month in data[year]:\n",
    "        month = str(month).zfill(2)\n",
    "        print(f\"Begin rename {year}-{month}\")\n",
    "        sdfs[f'sdf_{year}_{month}'] = rename(year, month)\n",
    "        print(f\"End rename {year}-{month}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking stuffs related to the original data (before cleaning + feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp_ntz (nullable = true)\n",
      " |-- on_scene_datetime: timestamp_ntz (nullable = true)\n",
      " |-- pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- pulocationid: integer (nullable = true)\n",
      " |-- dolocationid: integer (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: long (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# types\n",
    "sdfs['sdf_2023_12'].printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "121257739"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total records before cleaning\n",
    "total = 0\n",
    "for sdf in sdfs.values():\n",
    "    total += sdf.count()\n",
    "total    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers detecting and Feature selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Removing records based on the range of the field \n",
    "\n",
    "1. '2023-12-01' <= request_datetime <= on_scene_datetime <= pickup_datetime < dropoff_datetime < '2024-06-01'\n",
    "2. 1 <= pulocationid, dolocationid <= 263\n",
    "3. 0.1 <= trip_miles <= 360\n",
    "4. 0 <= trip_time <= 8*3600\n",
    "5. 7 <= base_passenger_fare\n",
    "6. 0 <= tolls, bcf, sales_tax, congestion_surcharge, airport_fee, tips, driver_pay\n",
    "7. Others only contain Y or N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_remove(sdf):\n",
    "    sdf = sdf.where(\n",
    "        (F.col('hvfhs_license_num') == 'HV0003') &\n",
    "        (F.col('request_datetime') >= '2023-12-01') &\n",
    "        (F.col('request_datetime') <= F.col('on_scene_datetime')) &\n",
    "        (F.col('on_scene_datetime') <= F.col('pickup_datetime')) &\n",
    "        (F.col('pickup_datetime') < F.col('dropoff_datetime')) &\n",
    "        (F.col('dropoff_datetime') < '2024-06-01') &\n",
    "        (F.col('pulocationid').between(1, 263)) &\n",
    "        (F.col('dolocationid').between(1, 263)) & \n",
    "        (F.col('trip_miles').between(0.1, 360)) &\n",
    "        (F.col('trip_time').between(0, 28800)) &\n",
    "        (F.col('base_passenger_fare') >= 7) &\n",
    "        (F.col('tolls') >= 0) &\n",
    "        (F.col('bcf') >= 0) &\n",
    "        (F.col('sales_tax') >= 0) &\n",
    "        (F.col('congestion_surcharge') >= 0) &\n",
    "        (F.col('tips') >= 0) &\n",
    "        (F.col('driver_pay') >= 0) &\n",
    "        (F.col('airport_fee') % 2.5 == 0) &\n",
    "        (F.col('shared_request_flag').isin('Y', 'N')) &\n",
    "        (F.col('shared_match_flag').isin('Y', 'N')) &\n",
    "        (F.col('access_a_ride_flag').isin('Y', 'N')) &\n",
    "        (F.col('wav_request_flag').isin('Y', 'N')) &\n",
    "        (F.col('wav_match_flag').isin('Y', 'N'))\n",
    "    )\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin sdf_2023_12\n",
      "End sdf_2023_12\n",
      "\n",
      "Begin sdf_2024_01\n",
      "End sdf_2024_01\n",
      "\n",
      "Begin sdf_2024_02\n",
      "End sdf_2024_02\n",
      "\n",
      "Begin sdf_2024_03\n",
      "End sdf_2024_03\n",
      "\n",
      "Begin sdf_2024_04\n",
      "End sdf_2024_04\n",
      "\n",
      "Begin sdf_2024_05\n",
      "End sdf_2024_05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, sdf in sdfs.items():\n",
    "    print(f'Begin {key}')\n",
    "    sdfs[key] = range_remove(sdf)\n",
    "    print(f'End {key}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep data in 0.1st to 99.9th approx percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_remove(sdf):\n",
    "    q_miles = sdf.approxQuantile(\"trip_miles\", [0.001, 0.999], 0.0001)\n",
    "    q_time = sdf.approxQuantile(\"trip_time\", [0.001, 0.999], 0.0001)\n",
    "    q_base_passenger_fare = sdf.approxQuantile(\"base_passenger_fare\", [0.001, 0.999], 0.0001)\n",
    "    q_tolls = sdf.approxQuantile(\"tolls\", [0.001, 0.999], 0.0001)\n",
    "    q_bcf = sdf.approxQuantile(\"bcf\", [0.001, 0.999], 0.0001)\n",
    "    q_sales_tax = sdf.approxQuantile(\"sales_tax\", [0.001, 0.999], 0.0001)\n",
    "    q_congestion_surcharge = sdf.approxQuantile(\"congestion_surcharge\", [0.001, 0.999], 0.0001)\n",
    "    q_tips = sdf.approxQuantile(\"tips\", [0.001, 0.999], 0.0001)\n",
    "    q_driver_pay = sdf.approxQuantile(\"driver_pay\", [0.001, 0.999], 0.0001)\n",
    "    sdf = sdf.where(\n",
    "        (F.col('trip_miles').between(q_miles[0], q_miles[1])) &\n",
    "        (F.col('trip_time').between(q_time[0], q_time[1])) &\n",
    "        (F.col('base_passenger_fare').between(q_base_passenger_fare[0], q_base_passenger_fare[1])) &\n",
    "        (F.col('tolls').between(q_tolls[0], q_tolls[1])) &\n",
    "        (F.col('bcf').between(q_bcf[0], q_bcf[1])) &\n",
    "        (F.col('sales_tax').between(q_sales_tax[0], q_sales_tax[1])) &\n",
    "        (F.col('congestion_surcharge').between(q_congestion_surcharge[0], q_congestion_surcharge[1])) &\n",
    "        (F.col('tips').between(q_tips[0], q_tips[1])) &\n",
    "        (F.col('driver_pay').between(q_driver_pay[0], q_driver_pay[1]))\n",
    "    )\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin sdf_2023_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End sdf_2023_12\n",
      "\n",
      "Begin sdf_2024_01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End sdf_2024_01\n",
      "\n",
      "Begin sdf_2024_02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End sdf_2024_02\n",
      "\n",
      "Begin sdf_2024_03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End sdf_2024_03\n",
      "\n",
      "Begin sdf_2024_04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End sdf_2024_04\n",
      "\n",
      "Begin sdf_2024_05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 158:=====================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End sdf_2024_05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for key, sdf in sdfs.items():\n",
    "    print(f'Begin {key}')\n",
    "    sdfs[key] = quantile_remove(sdf)\n",
    "    print(f'End {key}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 175:=====================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records before cleaning: 121257739\n",
      "Number of records after cleaning: 82592829\n",
      "There are  68.1134% of the original data kept\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Total records after cleaning\n",
    "total_c = 0\n",
    "for sdf in sdfs.values():\n",
    "    total_c += sdf.count()\n",
    "\n",
    "print(f\"Number of records before cleaning: {total}\")   \n",
    "print(f\"Number of records after cleaning: {total_c}\")\n",
    "print(f\"There are {total_c/total*100: .4f}% of the original data kept\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Splitting datetime into date and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_hour(sdf):\n",
    "    sdf = sdf.withColumn('day_of_week', dayofweek(sdf['request_datetime']))\n",
    "    sdf = sdf.withColumn('hour', hour(sdf['request_datetime']))\n",
    "    sdf = sdf.withColumn('is_weekend', when(sdf['day_of_week'].between(2, 6), 0).otherwise(1))\n",
    "    sdf = sdf.withColumn('date', to_date(sdf['request_datetime']))\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, sdf in sdfs.items():\n",
    "    sdfs[key] = date_hour(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_train = sdfs['sdf_2023_12']\n",
    "sdf_train = sdf_train.union(sdfs['sdf_2024_01'])\n",
    "sdf_train = sdf_train.union(sdfs['sdf_2024_02'])\n",
    "sdf_train = sdf_train.union(sdfs['sdf_2024_03'])\n",
    "sdf_train = sdf_train.union(sdfs['sdf_2024_04'])\n",
    "\n",
    "sdf_test = sdfs['sdf_2024_05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = spark.createDataFrame(pd.read_csv('../data/curated/weather_processed.csv'))\n",
    "weather = weather.select('wnd', 'tmp', 'dew', 'slp', 'date', 'hour')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join with weather data and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sdf_train.join(weather, [\"date\", \"hour\"], \"inner\")\n",
    "test_data = sdf_test.join(weather, [\"date\", \"hour\"], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/21 14:56:55 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/curated/train_data')\n",
    "    \n",
    "test_data \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/curated/test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
